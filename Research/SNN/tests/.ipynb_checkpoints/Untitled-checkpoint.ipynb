{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_X = T.tensor4(\"X\")\n",
    "input_shape = [None,1,28,28]\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fpga_sigmoid_simple(x): \n",
    "    abs_x = abs(x) \n",
    "    if (abs_x >= 5): \n",
    "        y = x/abs(x)/2+0.5\n",
    "    elif (abs_x >= 2.375): \n",
    "        y = 0.03125 * x/2 + np.sign(x)*0.84375 /2+0.5\n",
    "    elif (abs_x >= 1): \n",
    "        y = 0.125 * x/2 + np.sign(x)*0.625/2 +0.5 \n",
    "    else: \n",
    "        y = 0.25 * x/2 + np.sign(x)*0.5/2 +0.5 \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fpga_sigmoid_simple(x): \n",
    "    abs_x = np.abs(x) \n",
    "    if abs_x >= 5: \n",
    "        y = 1. \n",
    "    elif abs_x >= 2.375: \n",
    "        y = 0.03125 * abs_x + 0.84375 \n",
    "    elif abs_x >= 1: \n",
    "        y = 0.125 * abs_x + 0.625 \n",
    "    else: \n",
    "        y = 0.25 * abs_x + 0.5 \n",
    "    if (x < 0): \n",
    "        y = 1 - y \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1151f4110>]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHf1JREFUeJzt3Xt81PWd7/HXJ4FwvxPuCRdBIOCVCLjVekERbCvatVvQ\n09a2pz56dm13t7vbY9dWra3b2u7ZPduz7rbadt32AVpbq2KLC0FttbUEIirCcAsISbhMQrgGyP1z\n/pjBncaETJKZ/Obyfj4eeTDz+32T+fDLb97zy3d+8/mZuyMiIpklJ+gCREQk8RTuIiIZSOEuIpKB\nFO4iIhlI4S4ikoEU7iIiGUjhLiKSgRTuIiIZSOEuIpKB+gT1wKNHj/YpU6YE9fAiImnpjTfeOOLu\n+Z2NCyzcp0yZQllZWVAPLyKSlsxsfzzjNC0jIpKBFO4iIhlI4S4ikoEU7iIiGUjhLiKSgToNdzP7\nsZlVm9nWDtabmX3PzMrNbIuZXZ74MkVEpCviOXJ/AlhynvVLgRnRr7uBf+95WSIi0hOdnufu7q+a\n2ZTzDFkG/MQj1+vbYGbDzWy8ux9KUI0ikqKOnm7khbcPUlvXEHQpaWXR7LFcUjA8qY+RiA8xTQQq\nY+5XRZe9L9zN7G4iR/cUFhYm4KFFpLe5O5v2HWNl6X5efOcwjS2tmAVdVXoZM7R/WoR73Nz9MeAx\ngOLiYl2ZWySNnDjTxDObq1i1sYLy6jqG9O/DivkF3LFgMjPHDQm6PGkjEeF+ACiIuT8pukxE0py7\ns7niOKtKK/jVloM0NLdyacFwvnP7xXzk4gkMyMsNukTpQCLCfTVwj5k9BSwATmi+XSS9naxv4vk3\nD7CytIIdh08xKC+X2+dN4o4FhcyZMCzo8iQOnYa7mT0JXAuMNrMq4AGgL4C7fx9YA9wMlANngE8n\nq1gRSa4tVZGj9OffOsjZphbmThzKP9x2EbdcOoHB/QLrMyjdEM/ZMis6We/AXySsIhHpVacbmnn+\nrYOs2rifrQdOMqBvLrdcMoE7FxZy8aTkvuknyaOXYpEste3gifeO0usampk1bgjfWDaHZZdNZGj/\nvkGXJz2kcBfJImcbW3hhy0FWlVbwVuVx+vXJ4UMXj+fOBZO5vHA4pnMaM4bCXSQL7AqfYlVpBc9s\nruJUfTPTxwzm/g8X8dHLJzJ8YF7Q5UkSKNxFMlR9Uwsvbj3Eyg0VlO0/Rl5uDksvGscd8wuZP3Wk\njtIznMJdJMPsqal77yj9+Jkmpo4exN/fPIvb5xUwcpCO0rOFwl0kAzQ0t7B2W5hVpfvZsPcofXKM\nm+aM484FhSycNoqcHB2lZxuFu0ga2197mlUbK/hFWRW1pxspGDmALy+ZycfmFZA/pF/Q5UmAFO4i\naaappZX1oTArSyv4XfkRcnOMG2aP4Y4Fk7l6+mgdpQugcBdJG1XHzvDUxkp+VlZJzakGJgzrz5du\nvJCPX1HA2KH9gy5PUozCXSSF7QqfYsVjGwA4eqYRA66bOYY7FhRy7cwx5OooXTqgcBdJUa2tzld+\n+Q61pxsZ2r8PX7huOh+fX8jE4QOCLk3SgMJdJEX9YnMVb+w/xsO3zWX5FYU6SpcuUbiLpKCjpxv5\n1prtzJ8ykhVXFOpNUumyeC6QLSK97NsvbudUfTPfvG2ugl26ReEukmI27TvK02VV/M+rp3HhWF2+\nTrpH4S6SQppaWrnv2XeYOHwAX1w0PehyJI1pzl0khfzod++yK1zHDz9ZzMA8PT2l+3TkLpIiqo6d\n4V/W72Zx0VhuKBobdDmS5hTuIiniwdUhAB64ZU7AlUgmULiLpIB12w6zfnuYv75xhj6kJAmhcBcJ\n2OmGZh5cvY1Z44bw6Q9MDbocyRAKd5GAfe+l3Rw8Uc83b51L31w9JSUxtCeJBGjH4ZP88HfvsvyK\nAoqnjAy6HMkgCneRgLS2Ol99divDBvTlfy+ZFXQ5kmEU7iIB+fkblZTtP8ZXls5ihK5tKgmmcBcJ\nwNHTjXzrxR3MnzqS2+dNCrocyUAKd5EAfGvNdurqm3n41rmYqTGYJJ7CXaSXle6t5edvVPG5D05j\nhhqDSZIo3EV6UWNzK199biuTRgzgi9fPCLocyWDqTCTSi374u73srq7jR58qZkBebtDlSAaL68jd\nzJaY2U4zKzeze9tZX2hmr5jZm2a2xcxuTnypIumt8ugZvvfSbm6aM5ZFs9UYTJKr03A3s1zgUWAp\nUASsMLOiNsO+Cjzt7pcBy4F/S3ShIunM3Xlg9TZyzHjgI2oMJskXz5H7fKDc3fe6eyPwFLCszRgH\nhkZvDwMOJq5EkfS3dluYl3dU89c3XMgENQaTXhDPnPtEoDLmfhWwoM2YB4F1ZvYFYBBwQ0KqE8kA\npxua+foLkcZgd31gStDlSJZI1NkyK4An3H0ScDPwUzN73882s7vNrMzMympqahL00CKp7f+u38Wh\nE/U8fNtFagwmvSaePe0AUBBzf1J0WazPAk8DuPsfgP7A6LY/yN0fc/didy/Oz8/vXsUiaWT7oZP8\n+Pf7WDG/gHmTRwRdjmSReMJ9EzDDzKaaWR6RN0xXtxlTASwCMLPZRMJdh+aS1VpbnfuefUeNwSQQ\nnYa7uzcD9wBrge1EzorZZmYPmdkt0WF/A3zOzN4GngTucndPVtEi6eBnZZVsrjjO3988m+ED1RhM\neldcH2Jy9zXAmjbL7o+5HQI+kNjSRNLXkboGvv3iDhZMHcmfXj4x6HIkC+ndHZEk+NaaHZxpbObh\n29QYTIKhcBdJsD/sqeWZzVV87uppTB+jxmASDIW7SAI1NrfytecjjcG+oMZgEiA1DhNJoMdf20t5\ndR3/cdcVagwmgdKRu0iCNDa38v3f7uGG2WO5btaYoMuRLKdwF0mQ0ndrOVXfzMevKOh8sEiSKdxF\nEqQkFKZ/3xyumv6+D2eL9DqFu0gCuDvrQ2GunpGvuXZJCQp3kQTYdvAkB0/Uc2ORLsIhqUHhLpIA\n60JhcgwW6Y1USREKd5EEWLftMPMmj2DU4H5BlyICKNxFeqzy6Bl2HD6lKRlJKQp3kR4qCYUBuLFo\nXMCViPw3hbtID5WEwswYM5ipowcFXYrIexTuIj1w/EwjG/cd1ZSMpByFu0gPvLKzmpZWV7hLylG4\ni/RASSjMmCH9uGTS8KBLEfkjCneRbmpobuG3O2tYNHssOTm6IIekFoW7SDe9vqeW040tLNaUjKQg\nhbtIN5WEwgzMy+XKC0YFXYrI+yjcRbqhtTXSKOyaC/Pp31eNwiT1KNxFumHLgRNUn2rQWTKSshTu\nIt1QEjpMbo5xvRqFSYpSuIt0Q0kozBVTRjB8YF7QpYi0S+Eu0kX7a0+zK1ynXjKS0hTuIl10rlGY\nToGUVKZwF+midaEws8YNoWDkwKBLEemQwl2kC46ebqRMjcIkDSjcRbrg5R3VtDoKd0l5CneRLigJ\nHWbc0P5cNHFY0KWInJfCXSRO9U0tvLrrCDcWjcVMjcIktcUV7ma2xMx2mlm5md3bwZg/M7OQmW0z\ns1WJLVMkeL/bfYSzTS2akpG00KezAWaWCzwK3AhUAZvMbLW7h2LGzAC+AnzA3Y+ZmT62JxmnJBRm\nSL8+LJymRmGS+uI5cp8PlLv7XndvBJ4ClrUZ8zngUXc/BuDu1YktUyRYLa3OSzvCXDMzn7w+ms2U\n1BfPXjoRqIy5XxVdFutC4EIz+72ZbTCzJe39IDO728zKzKyspqamexWLBOCtymMcqWvUlIykjUQd\ngvQBZgDXAiuAx83sfdcdc/fH3L3Y3Yvz8/MT9NAiybcuFKZPjnHtTM04SnqIJ9wPAAUx9ydFl8Wq\nAla7e5O7vwvsIhL2IhmhJBRm4bRRDBvQN+hSROIST7hvAmaY2VQzywOWA6vbjHmOyFE7ZjaayDTN\n3gTWKRKYPTV17K05rSkZSSudhru7NwP3AGuB7cDT7r7NzB4ys1uiw9YCtWYWAl4B/s7da5NVtEhv\nOtco7AaFu6SRTk+FBHD3NcCaNsvuj7ntwJeiXyIZpSQUZs6EoUwcPiDoUkTipnO6RM6j5lQDmyuO\naUpG0o7CXeQ8Xt4RxtUoTNKQwl3kPEpCYSYOH0DR+KFBlyLSJQp3kQ6caWzmtd1qFCbpSeEu0oHX\ndh+hoblVUzKSlhTuIh0oCYUZ2r8P86eODLoUkS5TuIu0o6XVeXlHNdfNGkPfXD1NJP1orxVpxxv7\nj3H0dCOLi8YFXYpItyjcRdpREjpMXm4O18xUgztJTwp3kTbcnXWhMFdeMIrB/eL6ELdIylG4i7Sx\nu7qO/bVndJaMpDWFu0gb5xqFKdwlnSncRdpYFwpzyaRhjB3aP+hSRLpN4S4SI3yynrcrj+uoXdKe\nwl0kxvrt56ZkdAqkpDeFu0iMklCYwpEDuXDs4KBLEekRhbtIVF1DM6+X16pRmGQEhbtI1Ku7amhs\nUaMwyQwKd5GoklCY4QP7Ujx5RNCliPSYwl0EaGpp5eUd1Vw/awx91ChMMoD2YhFg076jnDjbxGJN\nyUiGULiLEJmSyeuTw9Uz1ChMMoPCXbKeu1MSCnPV9NEMUqMwyRAKd8l6Ow6fourYWZ0lIxlF4S5Z\nryQUxgwWzR4TdCkiCaNwl6xXEgpzWcFwxgxRozDJHAp3yWqHTpzlnQMn1EtGMo7CXbLaevVulwyl\ncJesti4UZtroQUwfo0ZhklkU7pK1TtY3sWFvrY7aJSPFFe5mtsTMdppZuZnde55xf2pmbmbFiStR\nJDl+u7OGphZXuEtG6jTczSwXeBRYChQBK8ysqJ1xQ4C/BEoTXaRIMqwLhRk1KI/LCtUoTDJPPEfu\n84Fyd9/r7o3AU8CydsZ9A3gEqE9gfSJJ0djcym92VLNo9hhyc9S7XTJPPOE+EaiMuV8VXfYeM7sc\nKHD3XyewNpGkKX23llMNzToFUjJWj99QNbMc4J+Av4lj7N1mVmZmZTU1NT19aJFuKwmF6d83h6um\njw66FJGkiCfcDwAFMfcnRZedMwSYC/zGzPYBC4HV7b2p6u6PuXuxuxfn56v7ngTD3VkfCnP1jHwG\n5OUGXY5IUsQT7puAGWY21czygOXA6nMr3f2Eu4929ynuPgXYANzi7mVJqVikh7YdPMnBE/U6S0Yy\nWqfh7u7NwD3AWmA78LS7bzOzh8zslmQXKJJo60JhcgwWzVKjMMlccTWvdvc1wJo2y+7vYOy1PS9L\nJHlKQmHmTR7BqMH9gi5FJGn0CVXJKpVHz7D90ElNyUjGU7hLVlm//VyjMJ0CKZlN4S5ZpSQUZsaY\nwUwdPSjoUkSSSuEuWePEmSZK3z2qKRnJCgp3yRqv7KympVWNwiQ7KNwla5SEwowZ0o9LJg0PuhSR\npFO4S1ZoaG7hNzurWTR7LDlqFCZZQOEuWeEPe2o53djCYk3JSJZQuEtWKAmFGZiXy5UXjAq6FJFe\noXCXjNfa6qzfHuaaC/Pp31eNwiQ7KNwl471z4AThkw06S0ayisJdMl5JKExujnG9GoVJFlG4S8Zb\nFzrMFVNGMHxgXtCliPQahbtktP21p9kVrlMvGck6CnfJaCWhSKMwnQIp2UbhLhltXSjMrHFDKBg5\nMOhSRHqVwl0y1tHTjZTtU6MwyU4Kd8lYL++optVRuEtWUrhLxioJHWbc0P5cNHFY0KWI9DqFu2Sk\n+qYWXt11hBuLxmKmRmGSfRTukpF+X36Es00tmpKRrKVwl4xUEgozpF8fFk5TozDJTgp3yTiRRmHV\nXDMzn7w+2sUlO2nPl4yz4d1ajtQ1cNMcfSpVspfCXTLOkxsrGZSXyw2zNd8u2UvhLhnlD3tqeeHt\ng9z1gSkMyFPvdsleCnfJGI3NrXzt+a0UjBzAPdfNCLockUD1CboAkUR5/LW9lFfX8R93XaGjdsl6\nOnKXjFBRe4bvvbSbpXPHcZ0uyiGicJf05+48sHorfXKM+z9SFHQ5IilB4S5pb+22w7yys4YvLZ7J\n+GEDgi5HJCXEFe5mtsTMdppZuZnd2876L5lZyMy2mNlLZjY58aWKvF9dQzMPrg5RNH4on7pSu53I\nOZ2Gu5nlAo8CS4EiYIWZtf3b902g2N0vBn4BfCfRhYq0559LdhE+Vc/Dt82lT67+EBU5J55nw3yg\n3N33unsj8BSwLHaAu7/i7meidzcAkxJbpsj7bTt4gide38cd8wu5rHBE0OWIpJR4wn0iUBlzvyq6\nrCOfBV5sb4WZ3W1mZWZWVlNTE3+VIm20tjr3PbuVEQP78uWbZgVdjkjKSejfsWb2P4Bi4LvtrXf3\nx9y92N2L8/PzE/nQkmWe3FTBW5XHue9Dsxk2sG/Q5YiknHg+xHQAKIi5Pym67I+Y2Q3AfcA17t6Q\nmPJE3u9IXQOPvLiDK6eN4tZLz/dHpEj2iufIfRMww8ymmlkesBxYHTvAzC4DfgDc4u7ViS9T5L/9\nw6+3c7aphW/cOldXWRLpQKfh7u7NwD3AWmA78LS7bzOzh8zsluiw7wKDgZ+b2VtmtrqDHyfSI6/v\nOcIv3zzA56+5gOljBgddjkjKiqu3jLuvAda0WXZ/zO0bElyXyPs0NLfw1ee2UjhyIH9x3fSgyxFJ\naWocJmnj8Vf3srfmNE98+gr691VjMJHz0ac+JC1U1J7h/71czocuGs+1M9UYTKQzCndJee7O156P\nNAb72ofVGEwkHgp3SXkvbj3Mb3fV8DeLZzJuWP+gyxFJCwp3SWmn6pv4+gvbmDNhKJ9UYzCRuOkN\nVUlp/1yym+pTDfzgE8VqDCbSBXq2SMraeuAET7z+LncuKOTSguFBlyOSVhTukpJaWp37ntvKyEF5\n/J0ag4l0mcJdUtKTGyt4u/I4X/1QEcMGqDGYSFcp3CXl1Jxq4JH/2sGfXDCKZZdOCLockbSkcJeU\n8/CvQzQ0taoxmEgPKNwlpbxefoTn3jrI56+ZxgX5agwm0l0Kd0kZ5xqDTR41kD9XYzCRHtF57pIy\nfvDbvew9cpr//Mx8NQYT6SEduUtK2HfkNP/6Sjkfung811yoSzCK9JTCXQJ3rjFYXm4O96sxmEhC\nKNwlcL9+5xCv7T7C3y6+kLFD1RhMJBE05y6BqG9q4ddbDvH4a3vZcfgUcycO5RNXTgm6LJGMoXCX\nXrWnpo5VpRX84o0qTpxtem/5N2+9iNwcndMukigKd0m6xuZW1m47zKrSCv6wt5a+ucZNc8Zx54LJ\nLJw2krNNLQzM064okkh6RknSVB49w6qNFfy8rJIjdY1MGjGALy+ZycfmFZA/pN974xTsIomnZ5Uk\nVHNLKy/vqGZlaQWv7q7BgEWzx3LngkI+OCOfHE29iPQKhbskxOET9Ty1qYKnNlZy+GQ9Y4f244vX\nz2D5/ALGDxsQdHkiWUfhLt3W2uq8uruGVaUVvLSjmpZW54MX5vP1ZXNYNGuMrpwkEiCFu3TZkboG\nni6r5MmNFVQePcuoQXnc/cFprLiikMJRA4MuT0RQuEuc3J0Ne4+ysnQ/a7cdpqnFWThtJF++aRaL\n54ylXx/1ghFJJQp3Oa/jZxp5ZvMBVpbuZ2/NaYb278MnFk7hjgWFTB+jlrwiqUrhLu/j7rxZeZyV\nGyr41ZaDNDS3clnhcP7xY5fw4YvHq2OjSBpQuMt76hqaee7NA6wsrWD7oZMMysvl9nmTuGNBIXMm\nDAu6PBHpAoW7sPXACVZtrOD5Nw9wurGFovFDefi2uSy7dCKD+2kXEUlHcT1zzWwJ8C9ALvBDd/92\nm/X9gJ8A84Ba4OPuvi+xpUoinW1s4YUtB1lZWsHblcfp3zeHj1w8gTsXTuaSScN07VKRNNdpuJtZ\nLvAocCNQBWwys9XuHooZ9lngmLtPN7PlwCPAx5NRsPTM7vApVpZW8MzmKk7VNzN9zGAe+EgRH71s\nEsMG9g26PBFJkHiO3OcD5e6+F8DMngKWAbHhvgx4MHr7F8C/mpm5uyewVummhuYW/mvrYVaWVrDx\n3aP0zTWWzh3PnQsKmT91pI7SRTJQPOE+EaiMuV8FLOhojLs3m9kJYBRwJBFFxnp6UyWPvba303Hx\nvK7E/coTx8B4f1a8r3fxjIr3pfPYmUZO1TdTOHIg9y6dxe3zJjF6cL/Ov1FE0lavvltmZncDdwMU\nFhZ262eMGJTHzLFD4nzAhAyJjIvz6DaeUfEeKMf3szof1b9vDkvnjueq6aPVuEskS8QT7geAgpj7\nk6LL2htTZWZ9gGFE3lj9I+7+GPAYQHFxcbembG4sGsuNRWO7860iIlkjns5Om4AZZjbVzPKA5cDq\nNmNWA5+K3r4deFnz7SIiwen0yD06h34PsJbIqZA/dvdtZvYQUObuq4EfAT81s3LgKJEXABERCUhc\nc+7uvgZY02bZ/TG364GPJbY0ERHpLjXcFhHJQAp3EZEMpHAXEclACncRkQykcBcRyUAW1OnoZlYD\n7O/mt48mCa0NEkB1dY3q6rpUrU11dU1P6prs7vmdDQos3HvCzMrcvTjoOtpSXV2jurouVWtTXV3T\nG3VpWkZEJAMp3EVEMlC6hvtjQRfQAdXVNaqr61K1NtXVNUmvKy3n3EVE5PzS9chdRETOI2XD3cw+\nZmbbzKzVzIrbrPuKmZWb2U4zu6mD759qZqXRcT+LtitOdI0/M7O3ol/7zOytDsbtM7N3ouPKEl1H\nO4/3oJkdiKnt5g7GLYluw3Izu7cX6vqume0wsy1m9qyZDe9gXK9sr87+/2bWL/o7Lo/uS1OSVUvM\nYxaY2StmForu/3/ZzphrzexEzO/3/vZ+VhJqO+/vxSK+F91eW8zs8l6oaWbMdnjLzE6a2V+1GdNr\n28vMfmxm1Wa2NWbZSDMrMbPd0X9HdPC9n4qO2W1mn2pvTJe4e0p+AbOBmcBvgOKY5UXA20A/YCqw\nB8ht5/ufBpZHb38f+F9Jrvf/APd3sG4fMLoXt92DwN92MiY3uu2mAXnRbVqU5LoWA32itx8BHglq\ne8Xz/wf+HPh+9PZy4Ge98LsbD1wevT0E2NVOXdcCv+qt/Sne3wtwM/AikYuILQRKe7m+XOAwkfPA\nA9lewAeBy4GtMcu+A9wbvX1ve/s9MBLYG/13RPT2iJ7UkrJH7u6+3d13trNqGfCUuze4+7tAOZGL\neL/HIteeu57IxboB/hO4NVm1Rh/vz4Ank/UYSfDehc/dvRE4d+HzpHH3de7eHL27gchVvYISz/9/\nGZF9ByL70iJL8tXE3f2Qu2+O3j4FbCdyjeJ0sAz4iUdsAIab2fhefPxFwB537+6HI3vM3V8lck2L\nWLH7UUdZdBNQ4u5H3f0YUAIs6UktKRvu59HeBbvb7vyjgOMxQdLemES6Ggi7++4O1juwzszeiF5H\ntjfcE/3T+Mcd/BkYz3ZMps8QOcprT29sr3j+/3904Xfg3IXfe0V0GugyoLSd1Vea2dtm9qKZzeml\nkjr7vQS9Ty2n4wOsILbXOWPd/VD09mGgveuEJnzb9eoFstsys/XAuHZW3efuz/d2Pe2Js8YVnP+o\n/Sp3P2BmY4ASM9sRfYVPSl3AvwPfIPJk/AaRKaPP9OTxElHXue1lZvcBzcDKDn5MwrdXujGzwcAz\nwF+5+8k2qzcTmXqoi76f8hwwoxfKStnfS/Q9tVuAr7SzOqjt9T7u7mbWK6coBhru7n5DN74tngt2\n1xL5k7BP9IirvTEJqdEiFwT/KDDvPD/jQPTfajN7lsiUQI+eFPFuOzN7HPhVO6vi2Y4Jr8vM7gI+\nDCzy6GRjOz8j4durHQm78HuimVlfIsG+0t1/2XZ9bNi7+xoz+zczG+3uSe2hEsfvJSn7VJyWApvd\nPdx2RVDbK0bYzMa7+6HoNFV1O2MOEHlv4JxJRN5v7LZ0nJZZDSyPnskwlcgr8MbYAdHQeIXIxboh\ncvHuZP0lcAOww92r2ltpZoPMbMi520TeVNza3thEaTPPeVsHjxfPhc8TXdcS4MvALe5+poMxvbW9\nUvLC79E5/R8B2939nzoYM+7c3L+ZzSfyPE7qi06cv5fVwCejZ80sBE7ETEckW4d/PQexvdqI3Y86\nyqK1wGIzGxGdRl0cXdZ9vfEOcne+iIRSFdAAhIG1MevuI3Kmw05gaczyNcCE6O1pREK/HPg50C9J\ndT4BfL7NsgnAmpg63o5+bSMyPZHsbfdT4B1gS3THGt+2ruj9m4mcjbGnl+oqJzKv+Fb06/tt6+rN\n7dXe/x94iMiLD0D/6L5THt2XpvXCNrqKyHTalpjtdDPw+XP7GXBPdNu8TeSN6T/phbra/b20qcuA\nR6Pb8x1iznJLcm2DiIT1sJhlgWwvIi8wh4CmaH59lsj7NC8Bu4H1wMjo2GLghzHf+5novlYOfLqn\ntegTqiIiGSgdp2VERKQTCncRkQykcBcRyUAKdxGRDKRwFxHJQAp3EZEMpHAXEclACncRkQz0/wEQ\ncRoU73n/GwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115011f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "y = [fpga_sigmoid_simple(i) for i in x]\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elemwise{add,no_inplace}.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(400).reshape(2, 200).astype(np.float32)\n",
    "r = theano.tensor.as_tensor(a)\n",
    "r*6+r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.62050301,  0.7849952 ,  0.55991471,  0.82318747,  0.79976267,\n",
       "         0.82607818,  0.79102427,  0.55443943,  0.69954336,  0.74440444,\n",
       "         0.7122184 ,  0.67490965,  0.75      ,  0.54501712,  0.54326934,\n",
       "         0.51057208,  0.63536471, -0.19465923,  0.53026921,  0.75      ,\n",
       "         0.58525974,  0.55556136,  0.58420104,  0.51439095,  0.79194379,\n",
       "         0.51192081,  0.75      ,  0.87453842,  0.55389506,  0.75      ,\n",
       "         0.50769889,  0.74536186,  0.58177447,  0.85318732,  0.75      ,\n",
       "         0.6777581 ,  0.50613207,  0.56243598,  0.58366561,  0.79575586,\n",
       "         0.75      ,  0.61712539,  0.70800555,  0.64530569,  0.51410145,\n",
       "         0.79504251,  0.65188891,  0.77178597,  0.81727171,  0.624479  ,\n",
       "         0.59955382,  0.75      ,  0.78076369,  0.75284332,  0.8021791 ,\n",
       "         0.61368233,  0.75968176,  0.58767462,  0.76751876,  0.66528565,\n",
       "         0.51554102,  0.54246277,  0.62923914,  0.53760052,  0.77196938,\n",
       "         0.55242926,  0.66360152,  0.56428289,  0.58500051,  0.54098976,\n",
       "         0.8735317 ,  0.77016455,  0.67844915,  0.5312022 ,  0.66656768,\n",
       "         0.78330564,  0.82273865,  0.60910767,  0.75357151,  0.64574361,\n",
       "         0.83643103,  0.65420514,  0.75      ,  0.83365977,  0.62589514,\n",
       "         0.55064404,  0.72603327,  0.73427963,  0.57598579,  0.79648983,\n",
       "         0.75070566,  0.52834433,  0.75      ,  0.57069826,  0.81749111,\n",
       "         0.62944502,  0.57882565,  0.60630089,  0.76063204,  0.81359982,\n",
       "         0.53654933,  0.73641586,  0.73334879,  0.79321694,  0.72870648,\n",
       "         0.60296059,  0.6351108 ,  0.85738897,  0.69316012,  0.75      ,\n",
       "         0.75017273,  0.67094994,  0.87020057,  0.84952176,  0.57396382,\n",
       "         0.54827422,  0.59438097,  0.84174907,  0.53868532,  0.66538918,\n",
       "         0.64625764,  0.53323936,  0.69005978,  0.71452129,  0.56435597,\n",
       "         0.747832  ,  0.80721754,  0.51697493,  0.50924373,  0.81991923,\n",
       "         0.60116673,  0.69172907,  0.67643768,  0.55683416,  0.58489817,\n",
       "         0.80271894,  0.73489755,  0.86142206,  0.66426051,  0.76898891,\n",
       "         0.66438776,  0.76008594,  0.54308033,  0.54558367,  0.68994576,\n",
       "         0.60755271,  0.68544894,  0.75      ,  0.52831066,  0.55730587,\n",
       "         0.66049671,  0.62249106,  0.62716556,  0.57727581,  0.80277252,\n",
       "         0.5939275 ,  0.61302114,  0.51949728,  0.77794796,  0.58508271,\n",
       "         0.76592886,  0.80217022,  0.66485804,  0.82214046,  0.64244974,\n",
       "         0.75      ,  0.70587426,  0.8153646 ,  0.83013493,  0.54525703,\n",
       "         0.61532927,  0.63727629,  0.66956401,  0.69758326,  0.69714749,\n",
       "         0.63305938,  0.64067894,  0.75      ,  0.50177348,  0.60578686,\n",
       "         0.77961391,  0.70197415,  0.71112937,  0.80973804,  0.53642231,\n",
       "         0.75      ,  0.61162424,  0.79753476,  0.71153194,  0.64202714,\n",
       "         0.69592744,  0.78852838,  0.54940975,  0.55714369,  0.58436936,\n",
       "         0.83773911,  0.79325378,  0.82840395,  0.53354901,  0.50526071],\n",
       "       [ 0.84568131,  0.7199595 ,  0.61241031,  0.56458354,  0.56912535,\n",
       "         0.59560221,  0.54193687,  0.7419458 ,  0.69694591,  0.7843222 ,\n",
       "         0.62876898,  0.85183132,  0.72486705,  0.51393664,  0.51152432,\n",
       "         0.823349  ,  0.67213506,  0.74915421,  0.55152839,  0.61621612,\n",
       "         0.76705277,  0.7179538 ,  0.64157069,  0.6998105 ,  0.5052526 ,\n",
       "         0.69109339,  0.78673619,  0.51096481,  0.59710383,  0.78286916,\n",
       "         0.51614702,  0.79650855,  0.75      ,  0.69631141,  0.73872089,\n",
       "         0.56433725,  0.63918757,  0.67000502,  0.56147557,  0.68211341,\n",
       "         0.87374961,  0.82836121,  0.66643906,  0.75875461,  0.60946429,\n",
       "         0.68384898,  0.72686231,  0.58637285,  0.53439492,  0.87125301,\n",
       "         0.65120417,  0.56834376,  0.56988478,  0.8219617 ,  0.50759536,\n",
       "         0.66198039,  0.7872265 ,  0.8744964 , -1.69886172,  0.68419415,\n",
       "         0.5860498 ,  0.6832397 ,  0.7210837 ,  0.65652502,  0.76154602,\n",
       "         0.5015775 ,  0.67269325,  0.57016861,  0.66374862,  0.85425544,\n",
       "         0.80118442,  0.76842755,  0.77796435,  0.64690721,  0.5336256 ,\n",
       "         0.57335889,  0.63475978,  0.78106707,  0.74797994,  0.83792388,\n",
       "         0.50938779,  0.68934238,  0.66024965,  0.56733304,  0.58645535,\n",
       "         0.79866362,  0.82949698,  0.64377189,  0.72079509,  0.74716508,\n",
       "         0.78688186,  0.81586629,  0.54833663,  0.60432076,  0.78599608,\n",
       "         0.82251424,  0.66041332,  0.52809411,  0.87239075,  0.66561496,\n",
       "         0.50544453,  0.60831249,  0.57149261,  0.65561962,  0.68440747,\n",
       "         0.64441192,  0.75598925,  0.7554034 ,  0.79703993,  0.6189478 ,\n",
       "         0.63999486,  0.53987712,  0.57759714,  0.72236496,  0.54730123,\n",
       "         0.5824315 ,  0.7545175 ,  0.82683975,  0.82772911,  0.67546588,\n",
       "         0.55844384,  0.84838724,  0.73352367,  0.75      ,  0.55939758,\n",
       "         0.71288729,  0.74848479,  0.52152872,  0.6783632 ,  0.69816923,\n",
       "         0.63649064,  0.8186627 ,  0.85449743,  0.58189207,  0.72851896,\n",
       "         0.70202392,  0.85122216,  0.79668218,  0.53228211,  0.71647966,\n",
       "         0.72390783,  0.59892279,  0.62284058,  0.8240273 ,  0.58704901,\n",
       "         0.57331407,  0.52040267,  0.75909048,  0.81245285,  0.78462952,\n",
       "         0.84044886,  0.5201512 ,  0.57448006,  0.55349761,  0.75340879,\n",
       "         0.58287245,  0.71133268,  0.78206837,  0.6376645 ,  0.75079703,\n",
       "         0.83850634,  0.75      ,  0.50746393,  0.51691967,  0.77076554,\n",
       "         0.76937264,  0.78021002,  0.74120808,  0.59881026,  0.57589245,\n",
       "         0.80276841,  0.73153025,  0.72648126,  0.52822596,  0.8081606 ,\n",
       "         0.52460104,  0.66464055,  0.62207228,  0.82551402,  0.58702111,\n",
       "         0.55608779,  0.85049105, -0.28680694,  0.75157619,  0.68750519,\n",
       "         0.64575094,  0.83508492,  0.63992798,  0.71935892,  0.64667064,\n",
       "         0.7086271 ,  0.85601711,  0.72832096,  0.63405174,  0.59844655,\n",
       "         0.67865145,  0.53416079,  0.56867516,  0.58215934,  0.78286088]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_non_lin(r).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.18974849954247475,\n",
       "  0.10750238597393036,\n",
       "  0.220042634755373,\n",
       "  0.088406264781951904,\n",
       "  0.10011867433786392,\n",
       "  0.086960919201374054,\n",
       "  0.89551212638616562,\n",
       "  0.77721970528364182,\n",
       "  0.84977166354656219,\n",
       "  0.87220223248004913,\n",
       "  0.14389079064130783,\n",
       "  0.16254518181085587,\n",
       "  0.93956969678401947,\n",
       "  0.22749144583940506,\n",
       "  0.2283653412014246,\n",
       "  0.75528604537248611,\n",
       "  0.81768235564231873,\n",
       "  0.039667196571826935,\n",
       "  0.7651346055790782,\n",
       "  0.94565382599830627,\n",
       "  0.79262986779212952,\n",
       "  0.2222193107008934,\n",
       "  0.20789947733283043,\n",
       "  0.75719547038897872,\n",
       "  0.89597187936306,\n",
       "  0.24403958208858967,\n",
       "  0.039581790566444397,\n",
       "  0.93726921081542969,\n",
       "  0.22305247932672501,\n",
       "  0.94147801399230957,\n",
       "  0.75384944956749678,\n",
       "  0.12731907516717911,\n",
       "  0.20911277085542679,\n",
       "  0.92659364640712738,\n",
       "  0.059279069304466248,\n",
       "  0.1611209437251091,\n",
       "  0.75306602055206895,\n",
       "  0.21878202259540558,\n",
       "  0.2081671804189682,\n",
       "  0.89787793904542923,\n",
       "  0.053553298115730286,\n",
       "  0.19143728911876678,\n",
       "  0.14599721878767014,\n",
       "  0.82265285402536392,\n",
       "  0.75705070933327079,\n",
       "  0.89752127230167389,\n",
       "  0.8259444534778595,\n",
       "  0.11410701274871826,\n",
       "  0.091364160180091858,\n",
       "  0.81223950162529945,\n",
       "  0.79977691546082497,\n",
       "  0.056050196290016174,\n",
       "  0.10961815714836121,\n",
       "  0.87642166018486023,\n",
       "  0.90108954906463623,\n",
       "  0.8068411760032177,\n",
       "  0.87984087318181992,\n",
       "  0.7938372939825058,\n",
       "  0.11624062806367874,\n",
       "  0.16735716909170151,\n",
       "  0.24222947750240564,\n",
       "  0.22876862622797489,\n",
       "  0.18538042902946472,\n",
       "  0.76880027167499065,\n",
       "  0.88598468154668808,\n",
       "  0.22378536313772202,\n",
       "  0.16819925606250763,\n",
       "  0.21785854175686836,\n",
       "  0.20749973505735397,\n",
       "  0.77049488760530949,\n",
       "  0.06323416531085968,\n",
       "  0.88508228212594986,\n",
       "  0.83922456204891205,\n",
       "  0.2343989135697484,\n",
       "  0.83328384906053543,\n",
       "  0.89165282249450684,\n",
       "  0.91136932373046875,\n",
       "  0.19544615224003792,\n",
       "  0.87678574025630951,\n",
       "  0.82287180423736572,\n",
       "  0.91821549832820892,\n",
       "  0.17289742827415466,\n",
       "  0.040069311857223511,\n",
       "  0.91682986915111542,\n",
       "  0.81294757872819901,\n",
       "  0.22467797249555588,\n",
       "  0.86301663517951965,\n",
       "  0.86713980883359909,\n",
       "  0.78799290955066681,\n",
       "  0.10175508260726929,\n",
       "  0.87535282969474792,\n",
       "  0.23582782130688429,\n",
       "  0.0553387850522995,\n",
       "  0.78534912317991257,\n",
       "  0.90874554961919785,\n",
       "  0.81472251564264297,\n",
       "  0.78941283375024796,\n",
       "  0.80315044149756432,\n",
       "  0.88031603395938873,\n",
       "  0.093200080096721649,\n",
       "  0.76827465742826462,\n",
       "  0.13179208338260651,\n",
       "  0.13332560658454895,\n",
       "  0.10339153558015823,\n",
       "  0.86435325443744659,\n",
       "  0.19851972162723541,\n",
       "  0.18244460970163345,\n",
       "  0.071305498480796814,\n",
       "  0.15341993421316147,\n",
       "  0.94851216673851013,\n",
       "  0.87508635222911835,\n",
       "  0.83547495305538177,\n",
       "  0.93510027974843979,\n",
       "  0.075239129364490509,\n",
       "  0.78698192164301872,\n",
       "  0.7741371113806963,\n",
       "  0.20280949771404266,\n",
       "  0.079125471413135529,\n",
       "  0.23065732605755329,\n",
       "  0.16730540245771408,\n",
       "  0.82312880456447601,\n",
       "  0.76661967486143112,\n",
       "  0.15497012436389923,\n",
       "  0.85726064443588257,\n",
       "  0.78217798098921776,\n",
       "  0.87391600757837296,\n",
       "  0.90360876172780991,\n",
       "  0.24151253513991833,\n",
       "  0.24537813384085894,\n",
       "  0.90995962172746658,\n",
       "  0.1994166262447834,\n",
       "  0.84586454182863235,\n",
       "  0.83821884542703629,\n",
       "  0.7784170676022768,\n",
       "  0.79244907945394516,\n",
       "  0.098640531301498413,\n",
       "  0.13255123049020767,\n",
       "  0.069288976490497589,\n",
       "  0.83213025331497192,\n",
       "  0.88449446111917496,\n",
       "  0.16780612617731094,\n",
       "  0.88004297018051147,\n",
       "  0.77154015563428402,\n",
       "  0.22720817476511002,\n",
       "  0.1550271138548851,\n",
       "  0.19622363522648811,\n",
       "  0.84272447228431702,\n",
       "  0.053621530532836914,\n",
       "  0.76415533106774092,\n",
       "  0.22134707495570183,\n",
       "  0.16975162923336029,\n",
       "  0.81124553456902504,\n",
       "  0.18641723692417145,\n",
       "  0.21136209741234779,\n",
       "  0.90138624608516693,\n",
       "  0.20303625613451004,\n",
       "  0.8065105564892292,\n",
       "  0.75974863395094872,\n",
       "  0.11102601140737534,\n",
       "  0.79254135489463806,\n",
       "  0.11703556030988693,\n",
       "  0.90108510106801987,\n",
       "  0.16757097840309143,\n",
       "  0.088929779827594757,\n",
       "  0.1787751168012619,\n",
       "  0.049265190958976746,\n",
       "  0.14706287533044815,\n",
       "  0.90768228471279144,\n",
       "  0.91506747156381607,\n",
       "  0.22737149707973003,\n",
       "  0.80766463652253151,\n",
       "  0.81863813847303391,\n",
       "  0.16521798819303513,\n",
       "  0.84879162162542343,\n",
       "  0.15142624825239182,\n",
       "  0.81652967631816864,\n",
       "  0.17966052144765854,\n",
       "  0.039615094661712646,\n",
       "  0.75088675157167017,\n",
       "  0.19710658118128777,\n",
       "  0.88980694860219955,\n",
       "  0.14901293814182281,\n",
       "  0.85556467622518539,\n",
       "  0.095130980014801025,\n",
       "  0.76821114867925644,\n",
       "  0.05579298734664917,\n",
       "  0.19418789073824883,\n",
       "  0.89876738935709,\n",
       "  0.14423402398824692,\n",
       "  0.82101357728242874,\n",
       "  0.84796371310949326,\n",
       "  0.89426419883966446,\n",
       "  0.77470487356185913,\n",
       "  0.77857183665037155,\n",
       "  0.20781530812382698,\n",
       "  0.081130445003509521,\n",
       "  0.89662688970565796,\n",
       "  0.085798025131225586,\n",
       "  0.23322549089789391,\n",
       "  0.24736963864415884],\n",
       " [0.077159345149993896,\n",
       "  0.85997976362705231,\n",
       "  0.1937948577105999,\n",
       "  0.78229176625609398,\n",
       "  0.78456266969442368,\n",
       "  0.20219888165593147,\n",
       "  0.77096842974424362,\n",
       "  0.87097290903329849,\n",
       "  0.8484729528427124,\n",
       "  0.89216110855340958,\n",
       "  0.81438448280096054,\n",
       "  0.9259156733751297,\n",
       "  0.8624335303902626,\n",
       "  0.75696833431720734,\n",
       "  0.75576216401532292,\n",
       "  0.088325485587120056,\n",
       "  0.83606753498315811,\n",
       "  0.8745771050453186,\n",
       "  0.22423580661416054,\n",
       "  0.80810805410146713,\n",
       "  0.88352638483047485,\n",
       "  0.85897690802812576,\n",
       "  0.17921466380357742,\n",
       "  0.84990525990724564,\n",
       "  0.75262631010264158,\n",
       "  0.84554669260978699,\n",
       "  0.10663189738988876,\n",
       "  0.75548240728676319,\n",
       "  0.79855191707611084,\n",
       "  0.10856541246175766,\n",
       "  0.75807350501418114,\n",
       "  0.10174570977687836,\n",
       "  0.045609697699546814,\n",
       "  0.15184428542852402,\n",
       "  0.13063955307006836,\n",
       "  0.78216863796114922,\n",
       "  0.18040621280670166,\n",
       "  0.83500251173973083,\n",
       "  0.78073779493570328,\n",
       "  0.1589432880282402,\n",
       "  0.063125200569629669,\n",
       "  0.08581940084695816,\n",
       "  0.16678045690059662,\n",
       "  0.87937732040882111,\n",
       "  0.80473215505480766,\n",
       "  0.84192449599504471,\n",
       "  0.86343114078044891,\n",
       "  0.20681357011198997,\n",
       "  0.76719745807349682,\n",
       "  0.93562649190425873,\n",
       "  0.82560209184885025,\n",
       "  0.21582812443375587,\n",
       "  0.2150576114654541,\n",
       "  0.089019156992435455,\n",
       "  0.24620232474990189,\n",
       "  0.16900978982448578,\n",
       "  0.10638675093650818,\n",
       "  0.93724820762872696,\n",
       "  0.97329539060592651,\n",
       "  0.15790291875600815,\n",
       "  0.20697510242462158,\n",
       "  0.15838013589382172,\n",
       "  0.86054185032844543,\n",
       "  0.82826249301433563,\n",
       "  0.11922699958086014,\n",
       "  0.2492112418403849,\n",
       "  0.16365335881710052,\n",
       "  0.21491570398211479,\n",
       "  0.83187431842088699,\n",
       "  0.072872273623943329,\n",
       "  0.099407792091369629,\n",
       "  0.11578622460365295,\n",
       "  0.11101783812046051,\n",
       "  0.8234536200761795,\n",
       "  0.76681280322372913,\n",
       "  0.21332055702805519,\n",
       "  0.81737988442182541,\n",
       "  0.10946646332740784,\n",
       "  0.12601003050804138,\n",
       "  0.081038057804107666,\n",
       "  0.75469388952478766,\n",
       "  0.84467118978500366,\n",
       "  0.16987516731023788,\n",
       "  0.78366651386022568,\n",
       "  0.7932276725769043,\n",
       "  0.10066817700862885,\n",
       "  0.91474848240613937,\n",
       "  0.17811406403779984,\n",
       "  0.13960245996713638,\n",
       "  0.12641745060682297,\n",
       "  0.10655906796455383,\n",
       "  0.90793313831090927,\n",
       "  0.22583168745040894,\n",
       "  0.19783963263034821,\n",
       "  0.89299803972244263,\n",
       "  0.088742874562740326,\n",
       "  0.83020665496587753,\n",
       "  0.76404705364257097,\n",
       "  0.06380462646484375,\n",
       "  0.16719251126050949,\n",
       "  0.75272227404639125,\n",
       "  0.80415625870227814,\n",
       "  0.21425369381904602,\n",
       "  0.82780980318784714,\n",
       "  0.15779627859592438,\n",
       "  0.17779404670000076,\n",
       "  0.87799461930990219,\n",
       "  0.12229828536510468,\n",
       "  0.89851995557546616,\n",
       "  0.80947389081120491,\n",
       "  0.18000257760286331,\n",
       "  0.23006144165992737,\n",
       "  0.78879855945706367,\n",
       "  0.13881751149892807,\n",
       "  0.22634937614202499,\n",
       "  0.79121574014425278,\n",
       "  0.87725874781608582,\n",
       "  0.086580120027065277,\n",
       "  0.91386454552412033,\n",
       "  0.8377329409122467,\n",
       "  0.2207780759781599,\n",
       "  0.075806364417076111,\n",
       "  0.86676184087991714,\n",
       "  0.044403418898582458,\n",
       "  0.22030121088027954,\n",
       "  0.14355634897947311,\n",
       "  0.1257576048374176,\n",
       "  0.7607643585652113,\n",
       "  0.83918161690235138,\n",
       "  0.84908462315797806,\n",
       "  0.81824532151222229,\n",
       "  0.90933134406805038,\n",
       "  0.072751268744468689,\n",
       "  0.79094602540135384,\n",
       "  0.86425947397947311,\n",
       "  0.8510119616985321,\n",
       "  0.07438892126083374,\n",
       "  0.8983410969376564,\n",
       "  0.23385892808437347,\n",
       "  0.14176016300916672,\n",
       "  0.86195391416549683,\n",
       "  0.20053859800100327,\n",
       "  0.81142028048634529,\n",
       "  0.91201366484165192,\n",
       "  0.20647550374269485,\n",
       "  0.21334297582507133,\n",
       "  0.23979866690933704,\n",
       "  0.87954524904489517,\n",
       "  0.093773573637008667,\n",
       "  0.89231476187705994,\n",
       "  0.079775556921958923,\n",
       "  0.76007558964192867,\n",
       "  0.78724003210663795,\n",
       "  0.22325118258595467,\n",
       "  0.87670439481735229,\n",
       "  0.20856376737356186,\n",
       "  0.14433367550373077,\n",
       "  0.89103417098522186,\n",
       "  0.18116775900125504,\n",
       "  0.87539850175380707,\n",
       "  0.9192531555891037,\n",
       "  0.049765825271606445,\n",
       "  0.75373196136206388,\n",
       "  0.24154015257954597,\n",
       "  0.11461722850799561,\n",
       "  0.11531367897987366,\n",
       "  0.1098949983716011,\n",
       "  0.12939596921205521,\n",
       "  0.79940513148903847,\n",
       "  0.78794620931148529,\n",
       "  0.90138419717550278,\n",
       "  0.86576511710882187,\n",
       "  0.86324063688516617,\n",
       "  0.235887018032372,\n",
       "  0.90408030897378922,\n",
       "  0.23769948538392782,\n",
       "  0.16767972707748413,\n",
       "  0.8110361322760582,\n",
       "  0.08724299818277359,\n",
       "  0.7935105636715889,\n",
       "  0.77804388478398323,\n",
       "  0.074754483997821808,\n",
       "  0.96691478043794632,\n",
       "  0.87578810006380081,\n",
       "  0.15624741464853287,\n",
       "  0.82287546992301941,\n",
       "  0.0824575275182724,\n",
       "  0.18003600090742111,\n",
       "  0.1403205469250679,\n",
       "  0.82333532720804214,\n",
       "  0.85431355237960815,\n",
       "  0.071991458535194397,\n",
       "  0.13583951443433762,\n",
       "  0.18297412246465683,\n",
       "  0.20077671855688095,\n",
       "  0.16067428141832352,\n",
       "  0.23291961476206779,\n",
       "  0.21566241607069969,\n",
       "  0.20892033725976944,\n",
       "  0.89143042266368866]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[fpga_sigmoid_simple(i) for i in j] for j in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def my_non_lin(x):\n",
    "    # x: tensor of shape (batch_size, number of features)\n",
    "    abs_x = abs(x)\n",
    "    fiv = abs_x // 5\n",
    "    tw = abs_x // 2.375\n",
    "    on = abs_x // 1\n",
    "    \n",
    "    y = fiv + (1-fiv)*tw*0.03125*x + (1-fiv)*tw*0.84375*np.sign(x) + (1-tw)*on*0.125*x +\\\n",
    "    (1-tw)*on*0.625*np.sign(x) +(1-on)*0.25*x + (1-on)*0.5*np.sign(x)\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "net = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X, name='input')\n",
    "net = lasagne.layers.DenseLayer(net, num_units = 50, nonlinearity = my_non_lin, name='hidden')\n",
    "net = lasagne.layers.DenseLayer(net, num_units = 10, nonlinearity = my_non_lin,\\\n",
    "                                name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'tens' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-0efdbe4b1492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjectives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjectives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/lasagne/layers/helper.pyc\u001b[0m in \u001b[0;36mget_output\u001b[0;34m(layer_or_layers, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m                                  \u001b[0;34m\"mapping this layer to an input expression.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                                  % layer)\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mall_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 accepted_kwargs |= set(utils.inspect_kwargs(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/lasagne/layers/dense.pyc\u001b[0m in \u001b[0;36mget_output_for\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonlinearity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-a8be62ce80fb>\u001b[0m in \u001b[0;36mmy_non_lin\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmy_non_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mtens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# x: tensor of shape (batch_size, number of features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mabs_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'tens' referenced before assignment"
     ]
    }
   ],
   "source": [
    "y_predicted = lasagne.layers.get_output(net)\n",
    "all_weights = lasagne.layers.get_all_params(net)\n",
    "\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "updates = lasagne.updates.adam(loss, all_weights,learning_rate=0.01)\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates)\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import math\n",
    "def iterate_minibatches(X, y, batchsize):\n",
    "    \n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(X) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield X[excerpt], y[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:04<06:42,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 4.053s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t8.99 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [00:07<06:05,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 of 100 took 2.948s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.21 %\n",
      "  validation accuracy:\t\t9.90 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/100 [00:09<05:33,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 of 100 took 2.748s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.71 %\n",
      "  validation accuracy:\t\t9.90 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/100 [00:12<05:07,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 of 100 took 2.676s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.94 %\n",
      "  validation accuracy:\t\t9.90 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 5/100 [00:15<04:53,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 of 100 took 2.792s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.94 %\n",
      "  validation accuracy:\t\t9.90 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/100 [00:18<04:51,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 of 100 took 3.143s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.79 %\n",
      "  validation accuracy:\t\t9.90 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/100 [00:24<05:58,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 of 100 took 5.617s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.94 %\n",
      "  validation accuracy:\t\t9.90 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 8/100 [00:32<07:54,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 of 100 took 8.177s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.89 %\n",
      "  validation accuracy:\t\t9.90 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 9/100 [00:39<08:48,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 of 100 took 7.300s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.73 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 10/100 [00:45<08:52,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 of 100 took 6.150s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 11/100 [00:48<07:23,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 of 100 took 2.808s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 12/100 [00:51<06:23,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 of 100 took 2.909s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 13/100 [00:54<05:40,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 of 100 took 2.878s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 14/100 [00:57<05:08,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 of 100 took 2.808s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 15/100 [01:00<04:54,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 of 100 took 3.193s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 16/100 [01:03<04:38,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 of 100 took 2.944s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 17/100 [01:06<04:21,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 of 100 took 2.784s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 18/100 [01:09<04:35,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 of 100 took 3.842s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 19/100 [01:12<04:18,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 of 100 took 2.807s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 20/100 [01:15<04:16,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 of 100 took 3.210s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 21/100 [01:18<04:05,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 of 100 took 2.896s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 22/100 [01:21<04:02,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 of 100 took 3.118s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 23/100 [01:24<03:52,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 of 100 took 2.793s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 24/100 [01:27<03:40,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 of 100 took 2.607s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 25/100 [01:30<03:33,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 100 took 2.711s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 26/100 [01:32<03:25,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 of 100 took 2.645s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 27/100 [01:35<03:19,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 of 100 took 2.600s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 28/100 [01:37<03:12,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 of 100 took 2.555s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 29/100 [01:40<03:10,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 of 100 took 2.699s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 30/100 [01:43<03:08,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 of 100 took 2.715s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 31/100 [01:45<03:05,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 of 100 took 2.650s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 32/100 [01:48<03:04,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 of 100 took 2.793s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 33/100 [01:51<02:59,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 of 100 took 2.587s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 34/100 [01:53<02:54,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 of 100 took 2.560s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 35/100 [01:56<02:51,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 of 100 took 2.621s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 36/100 [01:59<02:58,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 of 100 took 3.132s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 37/100 [02:02<03:02,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 of 100 took 3.141s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 38/100 [02:05<02:55,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 of 100 took 2.697s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.01 %\n",
      "  validation accuracy:\t\t9.15 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-796e158e5994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/tensor/blas.pyc\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   1542\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m             \u001b[0;31m# The error raised by numpy has no shape information, we mean to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 100 #количество проходов по данным\n",
    "\n",
    "batch_size = 50 #размер мини-батча\n",
    "\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "        \n",
    "        \n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    \n",
    "    train_accuracy.append(train_acc / train_batches * 100)\n",
    "    val_accuracy.append(val_acc / val_batches * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: колдун 99 уровня\")\n",
    "else:\n",
    "    print (\"Нужно больше магии!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
