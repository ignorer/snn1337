{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "from math import exp\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "class InputNeuron(object):\n",
    "    \n",
    "    def __init__(self, nnet, spike_train):\n",
    "        self.spike_train = spike_train\n",
    "        self.output_spikes_times = []\n",
    "        self.net = nnet\n",
    "        \n",
    "    def set_spike_train(self, spike_train):\n",
    "        self.spike_train = spike_train\n",
    "        self.output_spikes_times = []\n",
    "        \n",
    "    def step(self):\n",
    "        if self.spike_train[self.net.global_time] == 1:\n",
    "            self.output_spikes_times.append(self.net.global_time)\n",
    "    \n",
    "    def get_spikes(self):\n",
    "        return self.output_spikes_times\n",
    "\n",
    "class Neuron(object):\n",
    "    \n",
    "    def __init__(self, nnet, threshold=1., bias=0.):\n",
    "        self.input_spikes = [] # pairs time, intensity\n",
    "        self.output_spikes_times = []\n",
    "        self.net = nnet\n",
    "        self.potential = bias\n",
    "        self.threshold = threshold\n",
    "        self.tau_m = 4\n",
    "        self.tau_s = 2\n",
    "        self.tau_r = 20\n",
    "        self.time_scale = 1. # time unit is 100 ms\n",
    "        self.history = [0]\n",
    "        self.last_output_spikes_time = 0\n",
    "        self.bias = bias\n",
    "    \n",
    "    #очистить все после предыдущего запуска сети\n",
    "    def restart(self):\n",
    "        self.input_spikes = [] # pairs time, intensity\n",
    "        self.output_spikes_times = []\n",
    "        self.potential = 0\n",
    "        self.tau_m = 4\n",
    "        self.tau_s = 2\n",
    "        self.tau_r = 20\n",
    "        self.time_scale = 1. # time unit is 100 ms\n",
    "        self.history = [0]\n",
    "        self.last_output_spikes_time = 0\n",
    "    \n",
    "    def receive_spike(self, intensity):\n",
    "        self.input_spikes.append((self.net.global_time, intensity))\n",
    "    \n",
    "    def step(self):\n",
    "        self.potential = self.bias\n",
    "        self.spike = False\n",
    "\n",
    "        global_time = self.net.global_time\n",
    "        time_scale = self.time_scale\n",
    "        potential = 0\n",
    "        for spike_time, intensity in self.input_spikes:\n",
    "            if time_scale * (global_time - spike_time) < 30 and (intensity < -0.0001 or intensity > 0.0001):\n",
    "                potential += self.eps(global_time - spike_time) * intensity\n",
    "        self.potential = potential\n",
    "\n",
    "        self.potential += self.nu(self.net.global_time - self.last_output_spikes_time)\n",
    "\n",
    "        if self.potential > self.threshold:\n",
    "            self.input_spikes = []\n",
    "            self.output_spikes_times.append(self.net.global_time)\n",
    "            self.last_output_spikes_time = self.net.global_time\n",
    "            self.spike = True\n",
    "\n",
    "        self.history.append(self.potential)\n",
    "    \n",
    "    def eps(self, time):\n",
    "        if time <= 0:\n",
    "            return 0\n",
    "        s = -abs(time * self.time_scale)\n",
    "        return exp(s / self.tau_m) - exp(s / self.tau_s)\n",
    "\n",
    "    def grad_eps(self, time):\n",
    "        if time <= 0:\n",
    "            return 0\n",
    "        s = -abs(time * self.time_scale)\n",
    "        return exp(s / self.tau_m) * (-1 / self.tau_m) - exp(s / self.tau_s) * (-1 / self.tau_s)\n",
    "\n",
    "    def nu(self, time):\n",
    "        if time <= 0:\n",
    "            return 0\n",
    "        s = time * self.time_scale\n",
    "        return -self.threshold * exp(-abs(s) / self.tau_r) * (s > 0)\n",
    "\n",
    "    def get_spikes(self):\n",
    "        return self.output_spikes_times\n",
    "\n",
    "\n",
    "class Connection(object):\n",
    "    def __init__(self, nnet, input_neuron, output_neuron,\n",
    "                 weights=[1], delays=[1]):  # weights and delays are scaled\n",
    "        self.weights = weights\n",
    "        self.delays = delays\n",
    "        self.input_neuron = input_neuron\n",
    "        self.output_neuron = output_neuron\n",
    "        self.net = nnet\n",
    "        self.last_conducted_spike_index = 0\n",
    "\n",
    "    def step(self):\n",
    "        spikes = self.input_neuron.output_spikes_times\n",
    "        for i in range(self.last_conducted_spike_index, len(spikes)):\n",
    "            spike_time = spikes[i]\n",
    "            for j in range(len(self.weights)):\n",
    "                if spike_time + self.delays[j] == self.net.global_time:\n",
    "                    self.last_conducted_spike_index += 1\n",
    "                    self.output_neuron.receive_spike(self.weights[j])\n",
    "\n",
    "    def gradient(self, expected_spike_time):\n",
    "        output_spikes = np.array(self.output_neuron.get_spikes())\n",
    "        input_spikes = np.array(self.input_neuron.get_spikes())\n",
    "        assert len(output_spikes) > 0\n",
    "        output_spike_time = output_spikes[0]\n",
    "        eps = self.output_neuron.eps\n",
    "        grad_eps = self.output_neuron.grad_eps\n",
    "        grad_numerator = 0\n",
    "        grad_denumerator = 0\n",
    "        for i in range(len(input_spikes)):\n",
    "            grad_numerator += eps(output_spike_time - input_spikes[i] - self.delays)\n",
    "            # sum over all delays and input axons for output_neuron\n",
    "            grad_denumerator += self.weights * grad_eps(output_spike_time - input_spikes[i] - self.delays)\n",
    "        # leaning rate will be in a learning algorithm\n",
    "        grad = -grad_numerator / np.maximum(0.1, grad_denumerator) * (output_spike_time - expected_spike_time)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function fixed_frequency_spike_train at 0x10d223e60>\n"
     ]
    }
   ],
   "source": [
    "def fixed_frequency_spike_train(frequency, t_max):\n",
    "    actual_frequency = float(frequency)\n",
    "    result = [1 if frequency > 0 else 0]\n",
    "    for i in range(t_max - 1):\n",
    "        if actual_frequency >= 1:\n",
    "            result.append(1)\n",
    "            actual_frequency -= int(actual_frequency)\n",
    "        else:\n",
    "            result.append(0)\n",
    "        actual_frequency += frequency\n",
    "    return result\n",
    "\n",
    "from functools import reduce\n",
    "class InputLayer(object):\n",
    "    def __init__(self, nnet, shape):\n",
    "        self.net = nnet\n",
    "        self.shape = shape \n",
    "        self.neur_size = reduce(lambda res, x: res*x, self.shape, 1)\n",
    "        self.neurons = np.ndarray(shape=self.shape, dtype=InputNeuron, buffer=np.array([InputNeuron(self.net, []) for i in np.arange(self.neur_size)]))\n",
    "            \n",
    "    def random_spike_train(freq, t_max):\n",
    "        return sps.bernoulli.rvs(0.25*freq +0.5, size=t_max)\n",
    "    \n",
    "    def new_input(self, arg, t_max=1000, make_spike_train=fixed_frequency_spike_train):\n",
    "        for i, f in enumerate(arg):\n",
    "            for j, l in enumerate(f):\n",
    "                for k, m in enumerate(l):\n",
    "                    self.neurons[i][j][k].set_spike_train(fixed_frequency_spike_train(arg[i][j][k], t_max))\n",
    "    \n",
    "    def step(self):\n",
    "        for neur in self.neurons.reshape((self.neur_size)):\n",
    "            neur.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Conv2DLayer(object):\n",
    "    # Формат весов: w[nk][h][i][j], где nk - фильтр, h - номер фильтра на предыдущем слое, i, j - координаты весов в фильтре\n",
    "    def __init__(self, nnet, input_layer, num_filters, filter_shape, weights, threshold=1.):\n",
    "        self.net = nnet\n",
    "        self.filter_shape = filter_shape\n",
    "        self.weights = self.oldweights = weights.copy() \n",
    "        if(len(self.weights.shape) < 5):\n",
    "            self.weights = self.weights.reshape(np.append(weights.shape, 1))\n",
    "        \n",
    "        self.shape = (num_filters, input_layer.shape[1]-filter_shape[0]+1, input_layer.shape[2]-filter_shape[1]+1)\n",
    "        self.neur_size = reduce(lambda res, x: res*x, self.shape, 1)\n",
    "        self.neurons = np.array([Neuron(self.net, threshold=threshold) for i in np.arange(self.neur_size)]).reshape(self.shape)\n",
    "        \n",
    "        self.connections = []\n",
    "        \n",
    "        for nk, kernel in enumerate(self.neurons):\n",
    "            for i, row in enumerate(kernel):\n",
    "                for j, neuron in enumerate(row):   # соединяем с предыдущим слоем\n",
    "                    self.connections += [Connection(self.net, input_layer.neurons[l][i+p][j+q],neuron, self.weights[nk][l][p][q])\\\n",
    "                                         for l in np.arange(input_layer.shape[0]) for p in np.arange(filter_shape[0])\\\n",
    "                                         for q in np.arange(filter_shape[1])] \n",
    "                    \n",
    "    def restart(self):\n",
    "        for i, neur in enumerate(self.neurons.reshape((self.neur_size))):\n",
    "            neur.restart()\n",
    "    \n",
    "    def step(self):\n",
    "        for conn in self.connections:\n",
    "            conn.step()\n",
    "        for i, neur in enumerate(self.neurons.reshape((self.neur_size))):\n",
    "            neur.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SubSampling2DLayer(object):\n",
    "    def __init__(self, nnet, input_layer, pool_size, threshold=1.):\n",
    "        self.net = nnet\n",
    "        self.pool_size = pool_size\n",
    "        self.shape = input_layer.shape // np.append([1], pool_size)\n",
    "        self.neur_size = reduce(lambda res, x: res*x, self.shape, 1)\n",
    "        self.neurons = np.array([Neuron(self.net, threshold=threshold) for i in np.arange(self.neur_size)]).reshape(self.shape)\n",
    "        \n",
    "        self.conn_weight = 1 / (pool_size[0] * pool_size[1])\n",
    "        \n",
    "        self.connections = []\n",
    "        \n",
    "        for nk, kernel in enumerate(self.neurons):\n",
    "            for i, row in enumerate(kernel):\n",
    "                for j, neuron in enumerate(row):   \n",
    "                    self.connections += [Connection(self.net,input_layer.neurons[l][i*pool_size[0]+p][j*pool_size[1]+q],neuron,\\\n",
    "                                                    [self.conn_weight])\\\n",
    "                                         for l in np.arange(input_layer.shape[0]) for p in np.arange(pool_size[0])\\\n",
    "                                         for q in np.arange(pool_size[1])] \n",
    "                    \n",
    "    def restart(self):\n",
    "        for i, neur in enumerate(self.neurons.reshape((self.neur_size))):\n",
    "            neur.restart()\n",
    "        \n",
    "    def step(self):\n",
    "        for conn in self.connections:\n",
    "            conn.step()\n",
    "        for neur in self.neurons.reshape((self.neur_size)):\n",
    "            neur.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(5)\n",
    "class DenseLayer(object):\n",
    "    #Формат весов: w[i][j],  где i - номер нейрона на предыдущем слое, j - номер нейрона на текущем слое\n",
    "    def __init__(self,nnet, input_layer, num_units, weights, bias, threshold=1.):\n",
    "        self.net = nnet\n",
    "        self.shape = [num_units]\n",
    "        self.neur_size = num_units\n",
    "        self.neurons = np.array([Neuron(self.net, threshold, bias[i]) for i in np.arange(self.neur_size)])\n",
    "        self.weights = weights\n",
    "        \n",
    "        if(len(weights.shape) < 3):\n",
    "            weights = weights.reshape(np.append(weights.shape, 1))\n",
    "        \n",
    "        self.connections = [Connection(self.net, input_neuron, output_neuron, weights[i][j])\\\n",
    "                            for i, input_neuron in enumerate(input_layer.neurons.reshape((input_layer.neur_size)))\\\n",
    "                            for j, output_neuron in enumerate(self.neurons)]\n",
    "        \n",
    "    def restart(self):\n",
    "        for neur in self.neurons:\n",
    "            neur.restart()\n",
    "        \n",
    "    def step(self):\n",
    "        #for conn in self.connections:\n",
    "            #conn.step()\n",
    "        pool.map(lambda x: x.step(), self.connections)\n",
    "        pool.map(lambda x: x.step(), self.neurons)\n",
    "        #for neur in self.neurons:\n",
    "            #neur.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pool1 = ThreadPool(4)\n",
    "class NNet(object):\n",
    "    def __init__(self, shape, threshold=1.):\n",
    "        self.layers = [InputLayer(self, shape)]\n",
    "        self.global_time = 0\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def add_convolution(self, weights):\n",
    "        num_filters = weights.shape[0]\n",
    "        filter_shape = weights.shape[2:4]\n",
    "        self.layers.append(Conv2DLayer(self, self.layers[-1], num_filters, filter_shape, weights, threshold=self.threshold))\n",
    "        \n",
    "    def add_subsampling(self, pool_size):\n",
    "        self.layers.append(SubSampling2DLayer(self, self.layers[-1], pool_size, threshold=self.threshold))\n",
    "        \n",
    "    def add_dense(self, weights, bias, threshold=-1):\n",
    "        if(threshold == -1):\n",
    "            threshold = self.threshold\n",
    "        num_units = weights.shape[1]\n",
    "        self.layers.append(DenseLayer(self, self.layers[-1], num_units, weights, bias, threshold=threshold))\n",
    "    \n",
    "    def get_output_for(self, data, t_max):\n",
    "        self.global_time = 0\n",
    "        self.layers[0].new_input(data, t_max)\n",
    "        for l in self.layers[1:]:\n",
    "            l.restart()\n",
    "        for t in np.arange(t_max):\n",
    "            #for layer in self.layers:\n",
    "                #layer.step()\n",
    "            pool1.map(lambda x: x.step(), self.layers)\n",
    "            self.global_time += 1\n",
    "        result = [neur.get_spikes() for neur in self.layers[-1].neurons.reshape((self.layers[-1].neur_size))]\n",
    "        return result\n",
    "    \n",
    "    def classify(self, data, t_max):\n",
    "        self.global_time = 0\n",
    "        self.layers[0].new_input(data)\n",
    "        for l in self.layers[1:]:\n",
    "            l.restart()\n",
    "        ans = []\n",
    "        for t in np.arange(t_max):\n",
    "            #for layer in self.layers:\n",
    "                #layer.step()\n",
    "            self.pool.map(lambda x: x.step(), self.layers)\n",
    "            for i, neur in enumerate(self.layers[-1].neurons):\n",
    "                if len(neur.get_spikes()) > 0:\n",
    "                    ans.append(i)\n",
    "            if(len(ans) > 0):\n",
    "                return ans, t\n",
    "            self.global_time += 1\n",
    "        print('not_enough_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "def spiking_from_lasagne(input_net, threshold=1.):\n",
    "    input_layers = lasagne.layers.get_all_layers(input_net)\n",
    "    weights = lasagne.layers.get_all_param_values(input_net)\n",
    "    spiking_net = NNet(input_layers[0].shape[-3:], threshold)\n",
    "    if(len(threshold) == 1):\n",
    "        threshold = np.ones(len(input_layers)-1) * threshold\n",
    "    convert_layers = {lasagne.layers.conv.Conv2DLayer : spiking_net.add_convolution,\\\n",
    "                      lasagne.layers.dense.DenseLayer : spiking_net.add_dense}\n",
    "    \n",
    "    #номер элемента в общем массиве весов, в котором хранятся веса текущего слоя\n",
    "    i = 0\n",
    "    \n",
    "    for l in input_layers[1:]:\n",
    "        if(type(l) == lasagne.layers.pool.Pool2DLayer or type(l) == lasagne.layers.pool.MaxPool2DLayer):\n",
    "            spiking_net.add_subsampling(l.pool_size)\n",
    "        else:\n",
    "            convert_layers[type(l)](weights[i], bias=weights[i+1], threshold=threshold[i-1])\n",
    "            i+=2\n",
    "            \n",
    "    return spiking_net"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
