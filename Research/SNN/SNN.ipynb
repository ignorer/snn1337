{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "\n",
    "class InputNeuron(object):\n",
    "    \n",
    "    def __init__(self, nnet, spike_train):\n",
    "        self.spike_train = spike_train\n",
    "        self.output_spikes_times = []\n",
    "        self.net = nnet\n",
    "        \n",
    "    def set_spike_train(self, spike_train):\n",
    "        self.spike_train = spike_train\n",
    "        self.output_spikes_times = []\n",
    "        \n",
    "    def step(self):\n",
    "        if self.spike_train[self.net.global_time] == 1:\n",
    "            self.output_spikes_times.append(self.net.global_time)\n",
    "    \n",
    "    def get_spikes(self):\n",
    "        return self.output_spikes_times\n",
    "\n",
    "class Neuron(object):\n",
    "    \n",
    "    def __init__(self, nnet):\n",
    "        self.input_spikes = [] # pairs time, intensity\n",
    "        self.output_spikes_times = []\n",
    "        self.net = nnet\n",
    "        self.potential = 0\n",
    "        self.threshold = 1\n",
    "        self.tau_m = 4\n",
    "        self.tau_s = 2\n",
    "        self.tau_r = 20\n",
    "        self.time_scale = 1./9 # time unit is 100 ms\n",
    "        self.history = [0]\n",
    "    \n",
    "    #очистить все после предыдущего запуска сети\n",
    "    def restart(self):\n",
    "        self.input_spikes = [] # pairs time, intensity\n",
    "        self.output_spikes_times = []\n",
    "        self.history = [0]\n",
    "    \n",
    "    def receive_spike(self, intensity):\n",
    "        self.input_spikes.append((self.net.global_time, intensity))\n",
    "    \n",
    "    def step(self):\n",
    "        self.potential = 0\n",
    "        self.spike = False\n",
    "        \n",
    "        \n",
    "        for spike_time, intensity in self.input_spikes:\n",
    "            self.potential += self.eps(self.net.global_time - spike_time) * intensity\n",
    "            \n",
    "            \n",
    "        for spike_time in self.output_spikes_times:\n",
    "            self.potential += self.nu(self.net.global_time - spike_time)\n",
    "        \n",
    "        if self.potential > self.threshold:\n",
    "            self.output_spikes_times.append(self.net.global_time)\n",
    "            self.spike = True\n",
    "        \n",
    "        \n",
    "        self.history.append(self.potential)\n",
    "    \n",
    "    def eps(self, time):\n",
    "        s = time * self.time_scale # - delay\n",
    "        return (np.exp(-np.abs(s)/self.tau_m) - np.exp(-np.abs(s)/self.tau_s)) * (s > 0)\n",
    "    \n",
    "    def grad_eps(self, time):\n",
    "        s = time * self.time_scale\n",
    "        \n",
    "        return (np.exp(-np.abs(s)/self.tau_m) * (-1/self.tau_m) - np.exp(-np.abs(s)/self.tau_s) * (-1/self.tau_s)) * (s > 0)\n",
    "    \n",
    "    def nu(self, time):\n",
    "        s = time * self.time_scale\n",
    "        return -self.threshold * np.exp(-s/self.tau_r) * (s > 0)\n",
    "    \n",
    "    def get_spikes(self):\n",
    "        return self.output_spikes_times\n",
    "\n",
    "\n",
    "class Connection(object):\n",
    "\n",
    "    def __init__(self, nnet, input_neuron, output_neuron,\n",
    "                 weights=[1], delays=[1]): # weights and delays are scaled\n",
    "        self.weights = weights\n",
    "        self.delays = delays\n",
    "        self.input_neuron = input_neuron\n",
    "        self.output_neuron = output_neuron\n",
    "        self.net = nnet\n",
    "    \n",
    "    def step(self):\n",
    "        spikes = self.input_neuron.get_spikes()\n",
    "        for spike_time in spikes:\n",
    "            for weight, delay in zip(self.weights, self.delays):\n",
    "                if spike_time + delay == self.net.global_time:\n",
    "                    self.output_neuron.receive_spike(weight)\n",
    "    \n",
    "    def gradient(self, expected_spike_time):\n",
    "        output_spikes = np.array(self.output_neuron.get_spikes())\n",
    "        input_spikes = np.array(self.input_neuron.get_spikes())\n",
    "        assert len(output_spikes) > 0\n",
    "        output_spike_time = output_spikes[0]\n",
    "        eps = self.output_neuron.eps\n",
    "        grad_eps = self.output_neuron.grad_eps\n",
    "        grad_numerator = 0\n",
    "        grad_denumerator = 0\n",
    "        for i in range(len(input_spikes)):\n",
    "            grad_numerator += eps(output_spike_time - input_spikes[i] - self.delays)\n",
    "            grad_denumerator += self.weights * grad_eps(output_spike_time - input_spikes[i] - self.delays) # sum over all delays and input axons for output_neuron\n",
    "        grad = -grad_numerator / np.maximum(0.1, grad_denumerator) * (output_spike_time - expected_spike_time) # leaning rate will be in a learning algorithm\n",
    "        return grad\n",
    "    \n",
    "    ############################ Learning algoriphm\n",
    "    # ссылка на статью http://machinelearning.wustl.edu/mlpapers/paper_files/IM11.pdf стр.5\n",
    "    # https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-015-0059-4\n",
    "    def STDP():\n",
    "        output_spikes = np.array(self.output_neuron.get_spikes())\n",
    "        input_spikes = np.array(self.input_neuron.get_spikes())\n",
    "        weights = np.array(self.weights)\n",
    "        w_max = weights.max()\n",
    "        w_min = weights.min()\n",
    "        out_spikes_num = len(output_spikes)\n",
    "        assert out_spikes_num > 0\n",
    "        \n",
    "        tau_post_before = 0\n",
    "        tau_post_after = output_spikes[0]\n",
    "        out_ind = 0\n",
    "        \n",
    "        # для каждого синапса нужно поменять его вес\n",
    "        for j in range(len(weights)):\n",
    "            for i, tau_pre in enumerate(input_spikes):\n",
    "                ##################### тут нужно посмотреть, что делать со входящими спайками, которые пришли до первого \n",
    "                ##################### исходящего, и со спайками, которые пришли после после последнего исходящего\n",
    "                if (tau_pre > tau_post_after):\n",
    "                    tau_post_before = tau_post_after\n",
    "                    if (out_ind < out_spikes_num - 1):\n",
    "                        out_ind += 1\n",
    "                        tau_post_after = output_spikes[out_ind]\n",
    "                    else:\n",
    "                        tau_post_after = 0\n",
    "                delta_w = -1.0*etta*(self.tau_min-(tau_pre-tau_post_after))(0 <= tau_pre-tau_post_after <=tau_min)\\\n",
    "                        +etta*(self.tau_plus+(tau_pre-tau_post_after))(-1.*tau_plus <= tau_pre-tau_post_after <= 0)\\\n",
    "                         -1.0*etta*(self.tau_min-(tau_pre-tau_post_before))(0 <= tau_pre-tau_post_before <=tau_min)\\\n",
    "                        +etta*(self.tau_plus+(tau_pre-tau_post_before))(-1.*tau_plus <= tau_pre-tau_post_before <= 0)\n",
    "\n",
    "                w_old = self.weights[j]\n",
    "                self.weights[j] = w_old + sigma*delta_w*((w_max - w_old)*(delta_w > 0) + (w_old - w_min)*(delta_w <= 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "class InputLayer(object):\n",
    "    def __init__(self, nnet, shape):\n",
    "        self.net = nnet\n",
    "        self.shape = shape \n",
    "        self.neur_size = reduce(lambda res, x: res*x, self.shape, 1)\n",
    "        self.neurons = np.ndarray(shape=self.shape, dtype=InputNeuron, buffer=np.array([InputNeuron(self.net, []) for i in np.arange(self.neur_size)]))\n",
    "            \n",
    "    def make_spike_train(self, freq, t, t_max):\n",
    "        if t==0:\n",
    "            return np.zeros(t_max)\n",
    "        return sps.bernoulli.rvs(freq*t, size=t_max)\n",
    "    \n",
    "    def new_input(self, arg, t=100, t_max=1000):\n",
    "        for i, f in enumerate(arg):\n",
    "            for j, l in enumerate(f):\n",
    "                for k, m in enumerate(l):\n",
    "                    self.neurons[i][j][k].set_spike_train(make_spike_train(arg[i][j][k], t, t_max))\n",
    "    \n",
    "    def step(self):\n",
    "        for neur in self.neurons.reshape((self.neur_size)):\n",
    "            neur.step()\n",
    "            \n",
    "    def learning():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Conv2DLayer(object):\n",
    "    # Формат весов: w[nk][h][i][j], где nk - фильтр, h - номер фильтра на предыдущем слое, i, j - координаты весов в фильтре\n",
    "    def __init__(self, nnet, input_layer, num_filters, filter_shape, weights):\n",
    "        self.net = nnet\n",
    "        \n",
    "        self.weights = weights\n",
    "        if(len(self.weights.shape) < 5):\n",
    "            self.weights = self.weights.reshape(np.append(weights.shape, 1))\n",
    "        \n",
    "        self.shape = (num_filters, input_layer.shape[1]-filter_shape[0]+1, input_layer.shape[2]-filter_shape[1]+1)\n",
    "        self.neur_size = reduce(lambda res, x: res*x, self.shape, 1)\n",
    "        self.neurons = np.array([Neuron(self.net) for i in np.arange(self.neur_size)]).reshape(self.shape)\n",
    "        \n",
    "        self.connections = []\n",
    "        \n",
    "        for nk, kernel in enumerate(self.neurons):\n",
    "            for i, row in enumerate(kernel):\n",
    "                for j, neuron in enumerate(row):   # соединяем с предыдущим слоем\n",
    "                    self.connections += [Connection(self.net, input_layer.neurons[l][i+p][j+q],neuron, self.weights[nk][l][p][q])\\\n",
    "                                         for l in np.arange(input_layer.shape[0]) for p in np.arange(filter_shape[0])\\\n",
    "                                         for q in np.arange(filter_shape[1])] \n",
    "                    \n",
    "    def restart(self):\n",
    "        for i, neur in enumerate(self.neurons.reshape((self.neur_size))):\n",
    "            neur.restart()\n",
    "    \n",
    "    def step(self):\n",
    "        for conn in self.connections:\n",
    "            conn.step()\n",
    "        for i, neur in enumerate(self.neurons.reshape((self.neur_size))):\n",
    "            neur.step()\n",
    "            \n",
    "    def learning():\n",
    "        for conn in self.connections:\n",
    "            conn.STDP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SubSampling2DLayer(object):\n",
    "    def __init__(self, nnet, input_layer, pool_size):\n",
    "        self.net = nnet\n",
    "        self.shape = input_layer.shape // np.append([1], pool_size)\n",
    "        self.neur_size = reduce(lambda res, x: res*x, self.shape, 1)\n",
    "        self.neurons = np.array([Neuron(self.net) for i in np.arange(self.neur_size)]).reshape(self.shape)\n",
    "        \n",
    "        self.conn_weight = 1 / (pool_size[0] * pool_size[1])\n",
    "        \n",
    "        self.connections = []\n",
    "        \n",
    "        for nk, kernel in enumerate(self.neurons):\n",
    "            for i, row in enumerate(kernel):\n",
    "                for j, neuron in enumerate(row):   \n",
    "                    self.connections += [Connection(self.net,input_layer.neurons[l][i*pool_size[0]+p][j*pool_size[1]+q],neuron,\\\n",
    "                                                    [self.conn_weight])\\\n",
    "                                         for l in np.arange(input_layer.shape[0]) for p in np.arange(pool_size[0])\\\n",
    "                                         for q in np.arange(pool_size[1])] \n",
    "                    \n",
    "    def restart(self):\n",
    "        for i, neur in enumerate(self.neurons.reshape((self.neur_size))):\n",
    "            neur.restart()\n",
    "        \n",
    "    def step(self):\n",
    "        for conn in self.connections:\n",
    "            conn.step()\n",
    "        for neur in self.neurons.reshape((self.neur_size)):\n",
    "            neur.step()\n",
    "            \n",
    "    def learning():\n",
    "        for conn in self.connections:\n",
    "            conn.STDP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DenseLayer(object):\n",
    "    #Формат весов: w[i][j],  где i - номер нейрона на предыдущем слое, j - номер нейрона на текущем слое\n",
    "    def __init__(self,nnet, input_layer, num_units, weights):\n",
    "        self.net = nnet\n",
    "        self.shape = [num_units]\n",
    "        self.neur_size = num_units\n",
    "        self.neurons = np.array([Neuron(self.net) for i in np.arange(self.neur_size)])\n",
    "        \n",
    "        if(len(weights.shape) < 3):\n",
    "            weights = weights.reshape(np.append(weights.shape, 1))\n",
    "        \n",
    "        self.connections = [Connection(self.net, input_neuron, output_neuron, weights[i][j])\\\n",
    "                            for i, input_neuron in enumerate(input_layer.neurons.reshape((input_layer.neur_size)))\\\n",
    "                            for j, output_neuron in enumerate(self.neurons)]\n",
    "        \n",
    "    def restart(self):\n",
    "        for neur in self.neurons:\n",
    "            neur.restart()\n",
    "        \n",
    "    def step(self):\n",
    "        for conn in self.connections:\n",
    "            conn.step()\n",
    "        for neur in self.neurons:\n",
    "            neur.step()\n",
    "            \n",
    "    def learning():\n",
    "        for conn in self.connections:\n",
    "            conn.STDP()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NNet(object):\n",
    "    def __init__(self, shape):\n",
    "        self.layers = [InputLayer(self, shape)]\n",
    "        self.global_time = 0\n",
    "    \n",
    "    def add_convolution(self, weights):\n",
    "        num_filters = weights.shape[0]\n",
    "        filter_shape = weights.shape[2:4]\n",
    "        self.layers.append(Conv2DLayer(self, self.layers[-1], num_filters, filter_shape, weights))\n",
    "        \n",
    "    def add_subsampling(self, pool_size):\n",
    "        self.layers.append(SubSampling2DLayer(self, self.layers[-1], pool_size))\n",
    "        \n",
    "    def add_dense(self, weights):\n",
    "        num_units = weights.shape[1]\n",
    "        self.layers.append(DenseLayer(self, self.layers[-1], num_units, weights))\n",
    "    \n",
    "    def get_output_for(self, data, t_max):\n",
    "        self.global_time = 0\n",
    "        self.layers[0].new_input(data)\n",
    "        for l in self.layers[1:]:\n",
    "            l.restart()\n",
    "        for t in np.arange(t_max):\n",
    "            for layer in self.layers:\n",
    "                layer.step()\n",
    "            self.global_time += 1\n",
    "        result = [neur.get_spikes() for neur in self.layers[-1].neurons.reshape((self.layers[-1].neur_size))]\n",
    "        return result\n",
    "    \n",
    "    def classify(self, data, t_max):\n",
    "        self.global_time = 0\n",
    "        self.layers[0].new_input(data)\n",
    "        for l in self.layers[1:]:\n",
    "            l.restart()\n",
    "        ans = np.zeros(self.layers[-1].neur_size)\n",
    "        for t in np.arange(t_max):\n",
    "            for layer in self.layers:\n",
    "                layer.step()\n",
    "            for i, neur in enumerate(self.layers[-1].neurons):\n",
    "                if len(neur.getspikes()) > 0:\n",
    "                    ans.append(i)\n",
    "            if(len(ans) > 0):\n",
    "                return ans[0], t\n",
    "            self.global_time += 1\n",
    "            \n",
    "    def learning():\n",
    "        for layer in self.layers:\n",
    "            layer.learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning test on XOR function\n",
    "inputs = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "targets = np.array([[0.], [1.], [1.], [0.]])\n",
    "nn = NNet((1, 4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "\n",
    "def spiking_from_lasagne(input_net):\n",
    "    input_layers = lasagne.layers.get_all_layers(input_net)\n",
    "    weights = lasagne.layers.get_all_param_values(input_net)\n",
    "    spiking_net = NNet(input_layers[0].shape[-3:])\n",
    "    convert_layers = {lasagne.layers.conv.Conv2DLayer : spiking_net.add_convolution,\\\n",
    "                      lasagne.layers.dense.DenseLayer : spiking_net.add_dense}\n",
    "    \n",
    "    #номер элемента в общем массиве весов, в котором хранятся веса текущего слоя\n",
    "    i = 0\n",
    "    \n",
    "    for l in input_layers[1:]:\n",
    "        if(type(l) == lasagne.layers.pool.Pool2DLayer or type(l) == lasagne.layers.pool.MaxPool2DLayer):\n",
    "            spiking_net.add_subsampling(l.pool_size)\n",
    "        else:\n",
    "            convert_layers[type(l)](weights[i])\n",
    "            i+=2\n",
    "            \n",
    "    return spiking_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nn = NNet((1, 4, 4))\n",
    "nn.add_convolution(2, (3, 3), [[[1, 1, 1], [1, 0, 1], [0, 0, 1]], [[1, 1, 1], [1, 1, 1], [1, 1, 1]]])\n",
    "nn.add_subsampling((2, 2))\n",
    "nn.add_dense(1, [[100], [100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "inp = np.array([train]*16).reshape(1, 4, 4, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "input_X = T.tensor4(\"X\")\n",
    "input_shape = [None,1,28,28]\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "conv_1 = lasagne.layers.Conv2DLayer(input_layer, num_filters=2, filter_size=3, name='conv_1', \n",
    "                                    nonlinearity=lasagne.nonlinearities.rectify)\n",
    "max_pool_1 = lasagne.layers.MaxPool2DLayer(conv_1, pool_size=(2,2), name='max_pool_1')\n",
    "conv_2 = lasagne.layers.Conv2DLayer(max_pool_1, num_filters=3, filter_size=3, nonlinearity=lasagne.nonlinearities.rectify, \n",
    "                                    name='conv_2')\n",
    "max_pool_2 = lasagne.layers.MaxPool2DLayer(conv_2, pool_size=(2,2), name='max_pool_2')\n",
    "dense_output = lasagne.layers.DenseLayer(max_pool_2, num_units = 10,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')\n",
    "\n",
    "#print(max_pool_1.input_shape)\n",
    "l = lasagne.layers.get_all_layers(max_pool_2)\n",
    "max_pool_1.pool_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnnet = spiking_from_lasagne(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<__main__.InputLayer object at 0x107c2b650>, (1, 28, 28))\n",
      "(<__main__.Conv2DLayer object at 0x107cf0a90>, (2, 26, 26))\n",
      "(<__main__.SubSampling2DLayer object at 0x107cf0ad0>, array([ 2, 13, 13]))\n",
      "(<__main__.Conv2DLayer object at 0x107db1310>, (3, 11, 11))\n",
      "(<__main__.SubSampling2DLayer object at 0x1082c8950>, array([3, 5, 5]))\n",
      "(<__main__.DenseLayer object at 0x108419490>, [10])\n"
     ]
    }
   ],
   "source": [
    "for i in nnnet.layers:\n",
    "    print(i, i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3, 4, 6] * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as sps\n",
    "sps.bernoulli.rvs(90/255, size=255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot([1, 2], [2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = mnist.train.images.reshape(55000, 1, 28, 28)\n",
    "y_train = [np.dot(j, np.arange(10)) for j in mnist.train.labels]\n",
    "X_test = mnist.test.images.reshape(10000, 1, 28, 28)\n",
    "y_test = [np.dot(j, np.arange(10)) for j in mnist.test.labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "input_X = T.tensor4(\"X\")\n",
    "input_shape = [None,1,28,28]\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')\n",
    "input_layer = lasagne.layers.InputLayer(shape = input_shape,input_var=input_X)\n",
    "\n",
    "conv_1 = lasagne.layers.Conv2DLayer(input_layer, num_filters=8, filter_size=3, name='conv_1', \n",
    "                                    nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "max_pool_1 = lasagne.layers.Pool2DLayer(conv_1, pool_size=(2,2), name='max_pool_1', mode='average_inc_pad')\n",
    "\n",
    "conv_2 = lasagne.layers.Conv2DLayer(conv_1, num_filters=8, filter_size=3, nonlinearity=lasagne.nonlinearities.rectify, \n",
    "                                    name='conv_2')\n",
    "max_pool_2 = lasagne.layers.Pool2DLayer(conv_2, pool_size=(2,2), name='max_pool_2', mode='average_inc_pad')\n",
    "dense_output = lasagne.layers.DenseLayer(max_pool_2, num_units = 10,\n",
    "                                        nonlinearity = lasagne.nonlinearities.softmax,\n",
    "                                        name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-fbbbf59d4246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtrain_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-fbbbf59d4246>\u001b[0m in \u001b[0;36miterate_minibatches\u001b[0;34m(X, y, batchsize)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mexcerpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexcerpt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexcerpt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "y_predicted = lasagne.layers.get_output(dense_output)\n",
    "all_weights = lasagne.layers.get_all_params(dense_output)\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "updates_sgd = lasagne.updates.rmsprop(loss, all_weights,learning_rate=0.01)\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy)\n",
    "from random import shuffle\n",
    "import math\n",
    "def iterate_minibatches(X, y, batchsize):\n",
    "    \n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(X) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield X[excerpt], y[excerpt]\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 5 #количество проходов по данным\n",
    "\n",
    "batch_size = 30 #размер мини-батча\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "        \n",
    "        \n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
